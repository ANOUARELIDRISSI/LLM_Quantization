# üìä Rapport sur les Techniques de Quantization des LLMs

## üéØ Introduction

Ce rapport pr√©sente les diff√©rentes techniques de quantization utilis√©es pour optimiser les Large Language Models (LLMs). La quantization est une m√©thode de compression qui r√©duit la pr√©cision des poids du mod√®le pour diminuer l'utilisation de la m√©moire et acc√©l√©rer l'inf√©rence.

---

## üìê Qu'est-ce que la Quantization?

La quantization convertit les poids d'un mod√®le d'une repr√©sentation √† haute pr√©cision (comme FP32 ou FP16) vers une repr√©sentation √† plus faible pr√©cision (comme INT8 ou INT4).

### Formule de Quantization de Base

$$Q(x) = \text{round}\left(\frac{x - x_{min}}{x_{max} - x_{min}} \times (2^n - 1)\right)$$

O√π:
- $x$ = valeur originale
- $n$ = nombre de bits cible
- $Q(x)$ = valeur quantifi√©e

---

## üî¢ Types de Pr√©cision

| Type | Bits | Plage | Taille par param√®tre |
|------|------|-------|---------------------|
| **FP32** | 32 | ¬±3.4 √ó 10¬≥‚Å∏ | 4 bytes |
| **FP16** | 16 | ¬±65,504 | 2 bytes |
| **BF16** | 16 | ¬±3.4 √ó 10¬≥‚Å∏ | 2 bytes |
| **INT8** | 8 | -128 √† 127 | 1 byte |
| **INT4** | 4 | -8 √† 7 | 0.5 byte |
| **INT2** | 2 | -2 √† 1 | 0.25 byte |

---

## ‚ö° Technique 1: INT8 Quantization (8-bit)

### üìñ Explication

La quantization INT8 convertit les poids de 32 bits (FP32) ou 16 bits (FP16) vers 8 bits entiers. Cela r√©duit la taille du mod√®le de **4x** (FP32‚ÜíINT8) ou **2x** (FP16‚ÜíINT8).

### üî¨ Comment √ßa fonctionne

1. **Analyse des poids**: D√©terminer les valeurs min/max des poids
2. **Calcul du scale factor**: 
   $$\text{scale} = \frac{x_{max} - x_{min}}{255}$$
3. **Quantification**: Convertir chaque poids:
   $$x_{int8} = \text{round}\left(\frac{x - x_{min}}{\text{scale}}\right)$$
4. **Zero-point**: Calculer le point z√©ro pour la d√©-quantification

### üíª Types de Quantization INT8

#### A) Quantization Dynamique (Dynamic Quantization)
```python
# PyTorch Native
model_int8 = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # Couches √† quantifier
    dtype=torch.qint8
)
```
- Les poids sont quantifi√©s **√† l'avance**
- Les activations sont quantifi√©es **dynamiquement** pendant l'inf√©rence
- ‚úÖ Simple √† impl√©menter
- ‚úÖ Pas besoin de donn√©es de calibration
- ‚ö†Ô∏è Overhead de calcul pour les activations

#### B) Quantization Statique (Static Quantization)
```python
# N√©cessite une calibration
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
# Passer des donn√©es de calibration
calibrate(model, calibration_data)
torch.quantization.convert(model, inplace=True)
```
- Poids ET activations quantifi√©s **√† l'avance**
- N√©cessite un dataset de calibration
- ‚úÖ Plus rapide √† l'inf√©rence
- ‚ö†Ô∏è Plus complexe √† configurer

### üìä R√©sultats Typiques INT8

| M√©trique | FP32 | INT8 | Am√©lioration |
|----------|------|------|--------------|
| Taille mod√®le | 6 GB | 1.5 GB | **4x** plus petit |
| RAM requise | 8 GB | 3 GB | **2.6x** moins |
| Tokens/sec | 5 | 8-12 | **1.5-2x** plus rapide |
| Qualit√© | 100% | ~99% | Perte minimale |

---

## üöÄ Technique 2: INT4 Quantization (4-bit)

### üìñ Explication

La quantization INT4 pousse la compression encore plus loin en utilisant seulement 4 bits par poids. Cela permet une r√©duction de **8x** par rapport √† FP32.

### üî¨ Comment √ßa fonctionne

1. **Groupage des poids**: Les poids sont divis√©s en groupes (typiquement 32-128 poids)
2. **Scale par groupe**: Chaque groupe a son propre facteur d'√©chelle
3. **Quantification**: 
   $$x_{int4} = \text{round}\left(\frac{x}{\text{scale}}\right) + 8$$
   
   O√π les valeurs sont mapp√©es √† la plage [0, 15] ou [-8, 7]

### üíª Impl√©mentation avec BitsAndBytes

```python
from transformers import BitsAndBytesConfig

# Configuration INT4
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True  # Double quantization
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)
```

### üéØ Types de Quantization 4-bit

#### A) NF4 (NormalFloat4)
- Optimis√© pour les poids qui suivent une distribution normale
- Meilleure pr√©servation de la qualit√©
- Utilis√© par QLoRA

#### B) FP4 (Float4)
- Repr√©sentation flottante en 4 bits
- Plus flexible mais moins pr√©cis

### üìä R√©sultats Typiques INT4

| M√©trique | FP32 | INT4 | Am√©lioration |
|----------|------|------|--------------|
| Taille mod√®le | 6 GB | 0.75 GB | **8x** plus petit |
| RAM requise | 8 GB | 2 GB | **4x** moins |
| Tokens/sec | 5 | 6-10 | Variable |
| Qualit√© | 100% | ~95-98% | L√©g√®re perte |

---

## üßÆ Technique 3: GPTQ (Gradient-based Post-Training Quantization)

### üìñ Explication

GPTQ est une m√©thode de quantization post-entra√Ænement qui minimise l'erreur de reconstruction en utilisant des informations de gradient.

### üî¨ Algorithme

1. **Calcul de la matrice Hessienne**: Approximer les courbures des poids
2. **Quantification s√©quentielle**: Quantifier les colonnes une par une
3. **Compensation d'erreur**: Ajuster les poids non-quantifi√©s pour compenser

### üíª Code

```python
from transformers import GPTQConfig

gptq_config = GPTQConfig(
    bits=4,
    dataset="c4",
    tokenizer=tokenizer,
    group_size=128
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=gptq_config
)
```

### ‚úÖ Avantages
- Haute qualit√© de quantization
- Supporte 2, 3, 4, 8 bits
- Fonctionne bien sur GPU

### ‚ö†Ô∏è Inconv√©nients
- N√©cessite GPU pour la quantization
- Processus plus lent

---

## üéØ Technique 4: AWQ (Activation-aware Weight Quantization)

### üìñ Explication

AWQ identifie les poids "saillants" qui ont le plus d'impact sur les activations et les pr√©serve avec plus de pr√©cision.

### üî¨ Principe

1. **Analyse des activations**: Identifier quels poids affectent le plus les sorties
2. **Mise √† l'√©chelle**: Appliquer des scales diff√©rents selon l'importance
3. **Quantification adaptative**: Plus de pr√©cision pour les poids critiques

### üíª Code

```python
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_quantized(
    "model_name",
    fuse_layers=True,
    trust_remote_code=False
)
```

---

## üìà Technique 5: QLoRA (Quantized Low-Rank Adaptation)

### üìñ Explication

QLoRA combine la quantization 4-bit avec le fine-tuning efficace LoRA.

### üî¨ Comment √ßa fonctionne

1. **Mod√®le gel√© en 4-bit**: Le mod√®le de base est quantifi√© en NF4
2. **Adaptateurs LoRA**: Petites matrices entra√Ænables en FP16
3. **Double quantization**: Les constantes de quantization sont aussi quantifi√©es

### üíª Code

```python
from peft import LoraConfig, get_peft_model

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05
)

model = get_peft_model(model, lora_config)
```

---

## üîÑ Technique 6: GGUF/GGML Quantization

### üìñ Explication

Format de quantization optimis√© pour l'inf√©rence CPU, utilis√© par llama.cpp.

### üéØ Types de Quantization GGUF

| Type | Description | Taille | Qualit√© |
|------|-------------|--------|---------|
| Q2_K | 2-bit, super compact | ~0.3 GB/B | ‚≠ê‚≠ê |
| Q3_K_S | 3-bit, small | ~0.4 GB/B | ‚≠ê‚≠ê‚≠ê |
| Q4_K_M | 4-bit, medium | ~0.5 GB/B | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Q5_K_M | 5-bit, medium | ~0.6 GB/B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Q6_K | 6-bit | ~0.7 GB/B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Q8_0 | 8-bit | ~1 GB/B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### üíª Utilisation

```bash
# Conversion avec llama.cpp
./quantize model.gguf model-q4_k_m.gguf Q4_K_M
```

---

## üìä Comparaison des Techniques

| Technique | Bits | Compression | Qualit√© | GPU Requis | Complexit√© |
|-----------|------|-------------|---------|------------|------------|
| **INT8 Dynamic** | 8 | 4x | 99% | Non | ‚≠ê Facile |
| **INT8 Static** | 8 | 4x | 99% | Non | ‚≠ê‚≠ê Moyen |
| **INT4 BnB** | 4 | 8x | 95-98% | Oui | ‚≠ê‚≠ê Moyen |
| **GPTQ** | 2-8 | 4-16x | 96-99% | Oui | ‚≠ê‚≠ê‚≠ê Difficile |
| **AWQ** | 4 | 8x | 97-99% | Oui | ‚≠ê‚≠ê‚≠ê Difficile |
| **QLoRA** | 4 | 8x | 98%+ | Oui | ‚≠ê‚≠ê‚≠ê Difficile |
| **GGUF** | 2-8 | 4-16x | 90-99% | Non | ‚≠ê‚≠ê Moyen |

---

## üçì Compatibilit√© Raspberry Pi

### Mod√®les Recommand√©s par RAM

| RAM | Quantization | Taille Max Mod√®le | Exemple |
|-----|--------------|-------------------|---------|
| 4 GB | INT4/INT8 | ~1.5B params | Qwen2-1.5B-INT4 |
| 8 GB | INT4/INT8 | ~3B params | Llama-3.2-3B-INT4 |
| 16 GB | INT8 | ~7B params | Mistral-7B-INT8 |

### Performance Attendue

| Device | INT8 Tokens/sec | INT4 Tokens/sec |
|--------|----------------|-----------------|
| Raspberry Pi 5 (8GB) | 1-3 | 2-5 |
| Raspberry Pi 4 (8GB) | 0.5-1.5 | 1-3 |
| Desktop (16GB RAM) | 5-15 | 8-20 |

---

## üõ†Ô∏è Notre Impl√©mentation

### Projet: Quantization de Qwen2-1.5B-Instruct

Dans ce notebook, nous avons impl√©ment√©:

1. **Baseline FP16**: Mod√®le original en half-precision
2. **INT8 Dynamic**: Quantization PyTorch native
3. **INT4**: Quantization 4-bit pour compression maximale

### M√©triques Mesur√©es

- **Taille du mod√®le** (MB)
- **Ratio de compression**
- **Latence d'inf√©rence** (secondes)
- **Tokens par seconde**
- **Similarit√© Bigram** (qualit√© des r√©ponses)

---

## üìù Conclusion

La quantization est essentielle pour d√©ployer des LLMs sur des appareils √† ressources limit√©es:

- **INT8** offre un bon √©quilibre entre compression (4x) et qualit√©
- **INT4** maximise la compression (8x) avec une l√©g√®re perte de qualit√©
- **GPTQ/AWQ** offrent la meilleure qualit√© pour INT4
- **GGUF** est id√©al pour l'inf√©rence CPU sur edge devices

### Recommandations

| Cas d'usage | Technique Recommand√©e |
|-------------|----------------------|
| Production rapide | INT8 Dynamic |
| Edge device (Pi) | INT4 GGUF |
| Fine-tuning efficace | QLoRA |
| Qualit√© maximale INT4 | GPTQ ou AWQ |

---

---

## üîß Technique 7: Static INT8 Quantization

### üìñ Explication

La quantization statique pr√©-calcule les scales pour les activations en utilisant des donn√©es de calibration.

### üî¨ Comment √ßa fonctionne

1. **Pr√©paration**: Ins√©rer des observateurs (FakeQuantize) dans le mod√®le
2. **Calibration**: Passer des donn√©es repr√©sentatives
3. **Collecte**: Les observateurs enregistrent min/max des activations
4. **Conversion**: Convertir avec les statistiques collect√©es

### üíª Code

```python
# Pr√©parer le mod√®le
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

# Calibration avec donn√©es
for data in calibration_data:
    model(data)

# Convertir
torch.quantization.convert(model, inplace=True)
```

### ‚úÖ Avantages vs Dynamic
- Plus rapide √† l'inf√©rence (pas de calcul de scales)
- Mieux optimis√© pour le mat√©riel

---

## üîÑ Technique 8: Per-Channel Quantization

### üìñ Explication

Utilise un facteur d'√©chelle diff√©rent pour chaque canal de sortie.

### üî¨ Formule

$$W_{quant}[c] = \text{round}\left(\frac{W[c]}{\text{scale}[c]}\right)$$

O√π $c$ est l'indice du canal.

### ‚úÖ Avantages
- Meilleure pr√©cision que per-tensor
- G√®re mieux les variations de magnitude entre canaux

---

## ‚úÇÔ∏è Technique 9: Pruning + Quantization

### üìñ Explication

Combine deux techniques de compression :
1. **Pruning**: Mettre √† z√©ro les petits poids (sparsit√©)
2. **Quantization**: Quantifier les poids restants

### üíª Code

```python
import torch.nn.utils.prune as prune

# Appliquer pruning (50% des poids)
prune.l1_unstructured(module, name='weight', amount=0.5)

# Puis quantization
model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)
```

### üìä R√©sultats
- Compression: **5-8x**
- Sparsit√©: 50-90%
- Qualit√©: ~95%

---

## üéì Technique 10: Quantization-Aware Training (QAT)

### üìñ Explication

Simule la quantization pendant l'entra√Ænement pour que le mod√®le apprenne des poids robustes.

### üî¨ Comment √ßa fonctionne

```
Forward: x ‚Üí FakeQuantize ‚Üí Linear ‚Üí FakeQuantize ‚Üí ...
Backward: Gradients passent comme si pas de quantization (STE)
```

**Straight-Through Estimator (STE)**: Pendant la backprop, on ignore l'op√©ration de round().

### üíª Code

```python
class FakeQuantize(nn.Module):
    def forward(self, x):
        if self.training:
            x_q = torch.round(x / scale) * scale  # Fake quantize
            # STE: pretend no rounding for gradients
            return x + (x_q - x).detach()
        return x
```

---

## üì¶ Technique 11: ONNX Quantization

### üìñ Explication

Exporte le mod√®le au format ONNX puis applique la quantization avec ONNX Runtime.

### ‚úÖ Avantages
- Portabilit√© (CPU, GPU, mobile, edge)
- Optimisations sp√©cifiques au hardware
- Support INT8, INT4, mixte

### üíª Code

```python
# Export vers ONNX
torch.onnx.export(model, dummy_input, "model.onnx")

# Quantization ONNX
from onnxruntime.quantization import quantize_dynamic
quantize_dynamic("model.onnx", "model_int8.onnx", weight_type=QuantType.QInt8)
```

---

## üìä Tableau R√©capitulatif de Toutes les Techniques

| # | Technique | Compression | Qualit√© | GPU Requis | Complexit√© |
|---|-----------|-------------|---------|------------|------------|
| 1 | FP16 | 2x | 99.9% | Non | ‚≠ê |
| 2 | BF16 | 2x | 99.5% | Non | ‚≠ê |
| 3 | Dynamic INT8 | 4x | 99% | Non | ‚≠ê |
| 4 | Static INT8 | 4x | 99% | Non | ‚≠ê‚≠ê |
| 5 | Symmetric INT8 | 4x | 99% | Non | ‚≠ê |
| 6 | Asymmetric INT8 | 4x | 99.5% | Non | ‚≠ê‚≠ê |
| 7 | AbsMax INT8 | 4x | 99% | Non | ‚≠ê |
| 8 | MinMax INT8 | 4x | 99% | Non | ‚≠ê |
| 9 | Block-wise INT8 | 4x | 99.5% | Non | ‚≠ê‚≠ê |
| 10 | Per-Channel INT8 | 4x | 99.5% | Non | ‚≠ê‚≠ê |
| 11 | Histogram INT8 | 4x | 99% | Non | ‚≠ê‚≠ê |
| 12 | K-Means 4-bit | 8x | 95-98% | Non | ‚≠ê‚≠ê‚≠ê |
| 13 | K-Means 8-bit | 4x | 99% | Non | ‚≠ê‚≠ê‚≠ê |
| 14 | Mixed Precision | 2-4x | 99% | Non | ‚≠ê‚≠ê |
| 15 | QAT | 4x | 99%+ | Non | ‚≠ê‚≠ê‚≠ê |
| 16 | Pruning + INT8 | 5-8x | 95% | Non | ‚≠ê‚≠ê |
| 17 | ONNX INT8 | 4x | 99% | Non | ‚≠ê‚≠ê |
| 18 | INT4 BnB | 8x | 95-98% | Oui | ‚≠ê‚≠ê |
| 19 | GPTQ | 4-16x | 96-99% | Oui | ‚≠ê‚≠ê‚≠ê |
| 20 | AWQ | 8x | 97-99% | Oui | ‚≠ê‚≠ê‚≠ê |
| 21 | QLoRA | 8x | 98%+ | Oui | ‚≠ê‚≠ê‚≠ê |
| 22 | GGUF | 4-16x | 90-99% | Non | ‚≠ê‚≠ê |

---

## üÜï Nouvelles Techniques Ajout√©es

### BF16 (Brain Float16)
```
FP32:  1 sign | 8 exponent  | 23 mantissa = 32 bits
FP16:  1 sign | 5 exponent  | 10 mantissa = 16 bits  
BF16:  1 sign | 8 exponent  | 7 mantissa  = 16 bits
```
BF16 garde la m√™me plage que FP32 mais avec moins de pr√©cision.

### Symmetric vs Asymmetric INT8
- **Sym√©trique**: `scale = max(|x|) / 127`, zero_point = 0
- **Asym√©trique**: `scale = (max-min) / 255`, zero_point calcul√©

### Block-wise Quantization
Divise les poids en blocs avec des scales s√©par√©s:
```python
[w0...w63] ‚Üí scale_0
[w64...w127] ‚Üí scale_1
```

### K-Means Weight Clustering
Remplace les poids par des indices de clusters:
```python
weights ‚Üí K-Means(K=16) ‚Üí indices + 16 centroids
```

### Histogram/Percentile Clipping
Clip les outliers avant quantization:
```python
low = percentile(weights, 0.1%)
high = percentile(weights, 99.9%)
clipped = clip(weights, low, high)
```

---

## üõ†Ô∏è Mod√®les Sauvegard√©s dans ce Projet

| # | Mod√®le | Technique | Chemin |
|---|--------|-----------|--------|
| 1 | Qwen2-1.5B-Instruct | Dynamic INT8 | `Qwen2-1.5B-Instruct-INT8/` |
| 2 | Qwen2-1.5B-Instruct | FP16 | `Qwen2-1.5B-Instruct-FP16/` |
| 3 | Qwen2-1.5B-Instruct | BF16 | `Qwen2-1.5B-Instruct-BF16/` |
| 4 | Qwen2-1.5B-Instruct | Symmetric INT8 | `Qwen2-1.5B-Instruct-INT8-Symmetric/` |
| 5 | Qwen2-1.5B-Instruct | Asymmetric INT8 | `Qwen2-1.5B-Instruct-INT8-Asymmetric/` |
| 6 | Qwen2-1.5B-Instruct | Block-wise INT8 | `Qwen2-1.5B-Instruct-INT8-Blockwise-64/` |
| 7 | Qwen2-1.5B-Instruct | AbsMax INT8 | `Qwen2-1.5B-Instruct-INT8-AbsMax/` |
| 8 | Qwen2-1.5B-Instruct | MinMax INT8 | `Qwen2-1.5B-Instruct-INT8-MinMax/` |
| 9 | Qwen2-1.5B-Instruct | Histogram INT8 | `Qwen2-1.5B-Instruct-INT8-Histogram/` |
| 10 | Qwen2-1.5B-Instruct | Mixed Precision | `Qwen2-1.5B-Instruct-MixedPrecision/` |
| 11 | Qwen2-1.5B-Instruct | K-Means 4-bit | `Qwen2-1.5B-Instruct-KMeans-4bit/` |
| 12 | Qwen2-1.5B-Instruct | K-Means 8-bit | `Qwen2-1.5B-Instruct-KMeans-8bit/` |
| 13 | SimpleTransformer | Static INT8 | `static_int8_model/` |
| 14 | SimpleModel | ONNX INT8 | `onnx_quantized_model/` |
| 15 | PrunableModel | Pruning + INT8 | `pruned_quantized_model/` |
| 16 | SimpleModel | Per-Channel INT8 | `per_channel_quantized_model/` |
| 17 | QATModel | QAT + INT8 | `qat_quantized_model/` |

---

## üìö R√©f√©rences

1. Dettmers et al. "LLM.int8(): 8-bit Matrix Multiplication" (2022)
2. Frantar et al. "GPTQ: Accurate Post-Training Quantization" (2023)
3. Lin et al. "AWQ: Activation-aware Weight Quantization" (2023)
4. Dettmers et al. "QLoRA: Efficient Finetuning" (2023)
5. Han et al. "Deep Compression: Pruning, Quantization, Huffman Coding" (2016)
6. Jacob et al. "Quantization and Training of Neural Networks" (2018)

---

*Rapport g√©n√©r√© pour le projet de quantization LLM - Janvier 2026*
